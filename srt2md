#!/usr/bin/env python
import os
import re
import time
import logging
import collections
import json
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
from google import genai

load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("Error: GEMINI_API_KEY not found in .env file.")

BASE_SUBS_DIR = "deepgram"
BASE_MD_DIR = "md"

# API rate limiting
RPM_LIMIT = 5
DAILY_LIMIT = 1000

# List of models to try in order
MODEL_NAMES = [
    "gemini-2.5-flash-lite",
    "gemini-flash-lite-latest",
    "gemini-2.5-flash",
]

MODEL_STATE_FILE = ".transcribe_model_state.json"

# Track API request times for RPM
_api_request_times = collections.deque()

# Track daily request count and persist it
DAILY_COUNT_FILE = ".transcribe_daily_count.json"
def get_local_day():
    # Returns YYYY-MM-DD for local time, with day starting at 1am
    now = datetime.now()
    if now.hour == 0:
        # Before 1am, count as previous day
        return (now - timedelta(days=1)).strftime("%Y-%m-%d")
    return now.strftime("%Y-%m-%d")

def load_daily_count():
    if not os.path.exists(DAILY_COUNT_FILE):
        return {"date": get_local_day(), "count": 0}
    try:
        with open(DAILY_COUNT_FILE, "r") as f:
            data = json.load(f)
        # Reset if new day
        if data.get("date") != get_local_day():
            return {"date": get_local_day(), "count": 0}
        return data
    except Exception:
        return {"date": get_local_day(), "count": 0}

def save_daily_count(count):
    data = {"date": get_local_day(), "count": count}
    with open(DAILY_COUNT_FILE, "w") as f:
        json.dump(data, f)

# Setup logging
logging.basicConfig(
    filename="srt2md.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

client = genai.Client(api_key=API_KEY)


def load_model_state():
    if not os.path.exists(MODEL_STATE_FILE):
        return {"index": 0, "failures": [False] * len(MODEL_NAMES), "date": get_local_day()}
    try:
        with open(MODEL_STATE_FILE, "r") as f:
            data = json.load(f)
        # Reset failures on a new day
        if data.get("date") != get_local_day():
            return {"index": 0, "failures": [False] * len(MODEL_NAMES), "date": get_local_day()}
        if "failures" not in data or len(data["failures"]) != len(MODEL_NAMES):
            return {"index": 0, "failures": [False] * len(MODEL_NAMES), "date": get_local_day()}
        return data
    except Exception:
        return {"index": 0, "failures": [False] * len(MODEL_NAMES), "date": get_local_day()}


def save_model_state(state):
    state["date"] = get_local_day()
    with open(MODEL_STATE_FILE, "w") as f:
        json.dump(state, f)


def convert_to_markdown(transcript, file_path, max_retries=3):
    """
    Converts a plain transcript to structured markdown format using Gemini.
    """
    # Prepare markdown title (exclude video_id)
    base_name = file_path.stem.replace('_', ' ')
    video_id_pattern = re.compile(r'^[A-Za-z0-9_-]{11}$')
    tokens = base_name.split()
    # Remove last token if it matches video_id pattern
    if tokens and video_id_pattern.match(tokens[-1]):
        title_text = ' '.join(tokens[:-1])
    else:
        # Also check if last token after removing underscores matches
        base_name_no_underscore = file_path.stem.replace('_', '')
        if video_id_pattern.match(base_name_no_underscore[-11:]):
            title_text = base_name[:-12].rstrip()
        else:
            title_text = base_name
    
    global _api_request_times
    model_state = load_model_state()
    for model_try in range(len(MODEL_NAMES)):
        model_idx = model_state["index"]
        if model_state["failures"][model_idx]:
            model_state["index"] = (model_state["index"] + 1) % len(MODEL_NAMES)
            continue
        MODEL_NAME = MODEL_NAMES[model_idx]
        daily_data = load_daily_count()
        for attempt in range(1, max_retries + 1):
            # --- Daily quota enforcement ---
            if daily_data["count"] >= DAILY_LIMIT:
                print(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached for {daily_data['date']} on model {MODEL_NAME}. Switching model...")
                logging.warning(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached for {MODEL_NAME}. Switching model.")
                model_state["failures"][model_idx] = True
                save_model_state(model_state)
                break  # Try next model

            # --- Dynamic RPM limiting ---
            now = time.time()
            while _api_request_times and now - _api_request_times[0] > 60:
                _api_request_times.popleft()
            if len(_api_request_times) >= RPM_LIMIT:
                wait_time = 65 - (now - _api_request_times[0])
                wait_time = max(wait_time, 0)
                print(f"[Rate Limit] {len(_api_request_times)} requests in last 60s. Waiting {wait_time:.1f} seconds...")
                logging.info(f"[Rate Limit] Waiting {wait_time:.1f} seconds to avoid exceeding RPM limit.")
                time.sleep(wait_time)
            # Record this request
            _api_request_times.append(time.time())
            msg = f"Converting to markdown for {file_path.name} (Attempt {attempt}) with model {MODEL_NAME}..."
            print(msg)
            logging.info(msg)
            try:
                response = client.models.generate_content(
                    model=MODEL_NAME,
                    contents=[
                        f"Convert this transcript into a clean, structured Markdown study note. "
                        f"Use the title: '# {title_text}'.\n\n"
                        "**Strict Editing Rules for AI Optimization:**\n"
                        "1. **Executive Summary:** Start with a bulleted 'Executive Summary' of the 3-5 key scientific findings.\n"
                        "2. **Remove Visual References:** Delete phrases like 'black circles', 'as shown in the right panel', or 'you can see here'. Describe the data trend directly instead (e.g., 'The data shows a 50% increase').\n"
                        "3. **Format Protocols as Lists:** If the speaker describes a step-by-step protocol (like exercise routines or study methodology), format it as a clear bulleted list.\n"
                        "4. **Standardize Terms:** Replace 'lit mice' with 'Little (dwarf) mice' throughout. Ensure chemical names are spelled correctly.\n"
                        "5. **Delete Meta-Commentary:** Remove intros, outros, Patreon plugs, or calls to leave comments. Keep only the scientific data.\n"
                        "6. **Structure:** Use H2 headings (##) for major scientific topics (e.g., 'Mouse Data', 'Human Evidence').\n"
                        "7. **Visualizing Processes:** If a complex biological process is described (like a metabolic pathway), create a text-based flowchart using arrows ( -> ) to visualize the steps.\n\n"
                        f"Transcript:\n{transcript}"
                    ]
                )
                msg = f"Markdown conversion successful for {file_path} using {MODEL_NAME}"
                print(msg)
                logging.info(msg)
                daily_data["count"] += 1
                save_daily_count(daily_data["count"])
                # Reset model state to start with first model next time
                model_state["index"] = 0
                save_model_state(model_state)
                return response.text.strip() + '\n'
            except Exception as e:
                err_msg = f"Error converting to markdown for {file_path} on attempt {attempt} with {MODEL_NAME}: {e}"
                print(err_msg)
                logging.error(err_msg)

                error_str = str(e).lower()
                if 'rate limit' in error_str or 'quota' in error_str or '429' in error_str:
                    wait_time = 60
                    print(f"Rate/quota limit detected for {MODEL_NAME}. Waiting {wait_time} seconds...")
                    logging.warning(f"Rate/quota limit hit for {MODEL_NAME}. Waiting {wait_time} seconds.")
                    time.sleep(wait_time)
                    if 'quota' in error_str or 'daily' in error_str:
                        model_state["failures"][model_idx] = True
                        save_model_state(model_state)
                        break  # Try next model
                elif attempt < max_retries:
                    time.sleep(20)

                if attempt >= max_retries:
                    logging.error(f"Max retries reached for {file_path} with {MODEL_NAME}. Switching model.")
                    print(f"Max retries reached for {file_path} with {MODEL_NAME}. Switching model.")
                    model_state["failures"][model_idx] = True
                    save_model_state(model_state)
                    break  # Try next model
        # Move to next model
        model_state["index"] = (model_state["index"] + 1) % len(MODEL_NAMES)
        save_model_state(model_state)
    # If all models failed
    if all(model_state["failures"]):
        print("All models have exceeded their daily quota. Stopping.")
        logging.error("All models have exceeded their daily quota. Stopping.")
        import sys
        sys.exit(1)
    return None

def main():
    # Always start fresh with the first model on each run
    save_model_state({"index": 0, "failures": [False] * len(MODEL_NAMES)})

    msg = f"Scanning '{BASE_SUBS_DIR}' for transcript files..."
    print(msg)
    logging.info(msg)

    channels = []
    for entry in os.scandir(BASE_SUBS_DIR):
        if entry.is_dir():
            channels.append(entry.name)

    print(f"Found {len(channels)} channels.")
    logging.info(f"Found {len(channels)} channels.")

    processed = 0
    for channel in channels:
        channel_dir = Path(BASE_SUBS_DIR) / channel
        txt_files = sorted(channel_dir.glob("*.txt"))
        total_files = len(txt_files)
        print(f"Channel '{channel}': {total_files} transcript files.")
        logging.info(f"Channel '{channel}': {total_files} transcript files.")

        for idx, file_path in enumerate(txt_files, 1):
            relative_path = file_path.relative_to(BASE_SUBS_DIR)
            md_output_path = Path(BASE_MD_DIR) / relative_path.with_suffix('.md')

            if md_output_path.exists():
                msg = f"[{idx}/{total_files}] Markdown already exists, skipping: {file_path.name}"
                print(msg)
                logging.info(msg)
                continue

            print(f"[{idx}/{total_files}] Processing: {file_path.name}")
            logging.info(f"Processing {file_path.name} ({idx}/{total_files})")

            with open(file_path, "r", encoding="utf-8") as f:
                transcript = f.read()

            md_output_path.parent.mkdir(parents=True, exist_ok=True)
            markdown = convert_to_markdown(transcript, file_path)
            if markdown:
                with open(md_output_path, "w", encoding="utf-8") as f:
                    f.write(markdown)
                msg = f"Saved markdown: {md_output_path}"
                print(msg)
                logging.info(msg)
                processed += 1
            else:
                err_msg = f"Failed to create markdown for {file_path}"
                print(err_msg)
                logging.error(err_msg)

    print(f"All done! {processed} new markdown files created.")
    logging.info(f"All done! {processed} new markdown files created.")


if __name__ == "__main__":
    main()