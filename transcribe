#!/usr/bin/env python
import os
import sys
import time
import logging
from pathlib import Path
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("Error: GEMINI_API_KEY not found in .env file.")
    API_DELAY = 0   # No fixed delay; dynamic based on RPM
    RPM_LIMIT = 15  # Requests per minute
    import collections
    _api_request_times = collections.deque()

BASE_AUDIO_DIR = "audio"
BASE_SUBS_DIR = "subs"
BASE_MD_DIR = "md"

# API rate limiting (seconds between API calls)
API_DELAY = 20  # Used for fallback, but dynamic RPM and daily limit enforced below
RPM_LIMIT = 5  # Slightly below 15 to avoid bursts
DAILY_LIMIT = 1000
import collections
import json
from datetime import datetime, timedelta

# Track API request times for RPM
_api_request_times = collections.deque()

# Track daily request count and persist it
DAILY_COUNT_FILE = ".transcribe_daily_count.json"
def get_local_day():
    # Returns YYYY-MM-DD for local time, with day starting at 1am
    now = datetime.now()
    if now.hour == 0:
        # Before 1am, count as previous day
        return (now - timedelta(days=1)).strftime("%Y-%m-%d")
    return now.strftime("%Y-%m-%d")

def load_daily_count():
    if not os.path.exists(DAILY_COUNT_FILE):
        return {"date": get_local_day(), "count": 0}
    try:
        with open(DAILY_COUNT_FILE, "r") as f:
            data = json.load(f)
        # Reset if new day
        if data.get("date") != get_local_day():
            return {"date": get_local_day(), "count": 0}
        return data
    except Exception:
        return {"date": get_local_day(), "count": 0}

def save_daily_count(count):
    data = {"date": get_local_day(), "count": count}
    with open(DAILY_COUNT_FILE, "w") as f:
        json.dump(data, f)

# Setup logging
logging.basicConfig(
    filename="transcribe.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

client = genai.Client(api_key=API_KEY)


# List of models to try in order
MODEL_NAMES = [
    "gemini-2.5-flash-lite",
    "gemini-2.5-flash",
    "gemini-flash-lite-latest",
]

MODEL_STATE_FILE = ".transcribe_model_state.json"

def load_model_state():
    if not os.path.exists(MODEL_STATE_FILE):
        return {"index": 0, "failures": [False, False, False]}
    try:
        with open(MODEL_STATE_FILE, "r") as f:
            data = json.load(f)
        # Defensive: ensure correct length
        if "failures" not in data or len(data["failures"]) != len(MODEL_NAMES):
            return {"index": 0, "failures": [False]*len(MODEL_NAMES)}
        return data
    except Exception:
        return {"index": 0, "failures": [False]*len(MODEL_NAMES)}

def save_model_state(state):
    with open(MODEL_STATE_FILE, "w") as f:
        json.dump(state, f)

def transcribe_file_simple(file_path, max_retries=3):
    """
    Uploads an audio file to Gemini and returns a simple transcription.
    """
    # Map file extensions to mime types
    mime_types = {
        '.mp3': 'audio/mpeg',
        '.wav': 'audio/wav',
        '.m4a': 'audio/mp4',
        '.aac': 'audio/aac',
        '.flac': 'audio/flac',
        '.ogg': 'audio/ogg',
    }
    ext = Path(file_path).suffix.lower()
    mime_type = mime_types.get(ext, 'application/octet-stream')

    global _api_request_times
    model_state = load_model_state()
    for model_try in range(len(MODEL_NAMES)):
        model_idx = model_state["index"]
        if model_state["failures"][model_idx]:
            # Already failed for this model, skip
            model_state["index"] = (model_state["index"] + 1) % len(MODEL_NAMES)
            continue
        MODEL_NAME = MODEL_NAMES[model_idx]
        daily_data = load_daily_count()
        for attempt in range(1, max_retries + 1):
            # --- Daily quota enforcement ---
            if daily_data["count"] >= DAILY_LIMIT:
                print(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached for {daily_data['date']} on model {MODEL_NAME}. Switching model...")
                logging.warning(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached for {MODEL_NAME}. Switching model.")
                model_state["failures"][model_idx] = True
                save_model_state(model_state)
                break  # Try next model

            # --- Dynamic RPM limiting ---
            now = time.time()
            # Remove timestamps older than 60 seconds
            while _api_request_times and now - _api_request_times[0] > 60:
                _api_request_times.popleft()
            if len(_api_request_times) >= RPM_LIMIT:
                wait_time = 65 - (now - _api_request_times[0])
                wait_time = max(wait_time, 0)
                print(f"[Rate Limit] {len(_api_request_times)} requests in last 60s. Waiting {wait_time:.1f} seconds...")
                logging.info(f"[Rate Limit] Waiting {wait_time:.1f} seconds to avoid exceeding RPM limit.")
                time.sleep(wait_time)
            # Record this request
            _api_request_times.append(time.time())
            msg = f"Transcribing {file_path} (Attempt {attempt}) with model {MODEL_NAME}..."
            print(msg)
            logging.info(msg)
            try:
                with open(file_path, "rb") as f:
                    audio_data = f.read()

                audio_part = types.Part(
                    inline_data=types.Blob(
                        mime_type=mime_type,
                        data=audio_data
                    )
                )

                msg = f"Processing transcription for {file_path.name}..."
                print(msg)
                logging.info(msg)

                response = client.models.generate_content(
                    model=MODEL_NAME,
                    contents=[
                        audio_part,
                        "Transcribe this audio file accurately."
                    ]
                )

                msg = f"Transcription successful for {file_path} using {MODEL_NAME}"
                print(msg)
                logging.info(msg)
                # Increment and persist daily count
                daily_data["count"] += 1
                save_daily_count(daily_data["count"])
                # Reset model state to start with first model next time
                model_state["index"] = 0
                save_model_state(model_state)
                return response.text.strip()
            except Exception as e:
                err_msg = f"Error transcribing {file_path} on attempt {attempt} with {MODEL_NAME}: {e}"
                print(err_msg)
                logging.error(err_msg)

                # Check for rate limit or quota errors
                error_str = str(e).lower()
                if 'rate limit' in error_str or 'quota' in error_str or '429' in error_str:
                    wait_time = 60  # Wait 60 seconds for rate limit errors
                    print(f"Rate/quota limit detected for {MODEL_NAME}. Waiting {wait_time} seconds...")
                    logging.warning(f"Rate/quota limit hit for {MODEL_NAME}. Waiting {wait_time} seconds.")
                    time.sleep(wait_time)
                    if 'quota' in error_str or 'daily' in error_str:
                        # Mark this model as failed for quota
                        model_state["failures"][model_idx] = True
                        save_model_state(model_state)
                        break  # Try next model
                elif attempt < max_retries:
                    time.sleep(20)

                if attempt >= max_retries:
                    logging.error(f"Max retries reached for {file_path} with {MODEL_NAME}. Switching model.")
                    print(f"Max retries reached for {file_path} with {MODEL_NAME}. Switching model.")
                    model_state["failures"][model_idx] = True
                    save_model_state(model_state)
                    break  # Try next model
        # Move to next model
        model_state["index"] = (model_state["index"] + 1) % len(MODEL_NAMES)
        save_model_state(model_state)
    # If all models failed
    if all(model_state["failures"]):
        print("All models have exceeded their daily quota. Stopping.")
        logging.error("All models have exceeded their daily quota. Stopping.")
        sys.exit(1)
    return None

def convert_to_markdown(transcript, file_path, max_retries=3):
    """
    Converts a plain transcript to structured markdown format using Gemini.
    """
    import re
    # Prepare markdown title (exclude video_id)
    base_name = file_path.stem.replace('_', ' ')
    video_id_pattern = re.compile(r'^[A-Za-z0-9_-]{11}$')
    tokens = base_name.split()
    # Remove last token if it matches video_id pattern
    if tokens and video_id_pattern.match(tokens[-1]):
        title_text = ' '.join(tokens[:-1])
    else:
        # Also check if last token after removing underscores matches
        base_name_no_underscore = file_path.stem.replace('_', '')
        if video_id_pattern.match(base_name_no_underscore[-11:]):
            title_text = base_name[:-12].rstrip()
        else:
            title_text = base_name
    
    global _api_request_times
    daily_data = load_daily_count()
    for attempt in range(1, max_retries + 1):
        # --- Daily quota enforcement ---
        if daily_data["count"] >= DAILY_LIMIT:
            print(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached for {daily_data['date']}. Waiting for reset at 1am local time.")
            logging.warning(f"[DAILY LIMIT] {DAILY_LIMIT} requests reached. Waiting for reset at 1am local time.")
            # Sleep until 1:01am local time
            now = datetime.now()
            tomorrow = now + timedelta(days=1)
            reset_time = datetime(year=tomorrow.year, month=tomorrow.month, day=tomorrow.day, hour=1, minute=1)
            wait_seconds = (reset_time - now).total_seconds()
            print(f"Sleeping for {int(wait_seconds)} seconds until quota reset...")
            time.sleep(wait_seconds)
            daily_data = {"date": get_local_day(), "count": 0}
            save_daily_count(0)

        # --- Dynamic RPM limiting ---
        now = time.time()
        # Remove timestamps older than 60 seconds
        while _api_request_times and now - _api_request_times[0] > 60:
            _api_request_times.popleft()
        if len(_api_request_times) >= RPM_LIMIT:
            wait_time = 65 - (now - _api_request_times[0])
            wait_time = max(wait_time, 0)
            print(f"[Rate Limit] {len(_api_request_times)} requests in last 60s. Waiting {wait_time:.1f} seconds...")
            logging.info(f"[Rate Limit] Waiting {wait_time:.1f} seconds to avoid exceeding RPM limit.")
            time.sleep(wait_time)
        # Record this request
        _api_request_times.append(time.time())
        msg = f"Converting to markdown for {file_path.name} (Attempt {attempt})..."
        print(msg)
        logging.info(msg)
        try:
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=[
                    f"Convert this transcript into a clean, structured Markdown study note. "
                    f"Use the title: '# {title_text}'.\n\n"
                    "**Strict Editing Rules for AI Optimization:**\n"
                    "1. **Executive Summary:** Start with a bulleted 'Executive Summary' of the 3-5 key scientific findings.\n"
                    "2. **Remove Visual References:** Delete phrases like 'black circles', 'as shown in the right panel', or 'you can see here'. Describe the data trend directly instead (e.g., 'The data shows a 50% increase').\n"
                    "3. **Format Protocols as Lists:** If the speaker describes a step-by-step protocol (like exercise routines or study methodology), format it as a clear bulleted list.\n"
                    "4. **Standardize Terms:** Replace 'lit mice' with 'Little (dwarf) mice' throughout. Ensure chemical names are spelled correctly.\n"
                    "5. **Delete Meta-Commentary:** Remove intros, outros, Patreon plugs, or calls to leave comments. Keep only the scientific data.\n"
                    "6. **Structure:** Use H2 headings (##) for major scientific topics (e.g., 'Mouse Data', 'Human Evidence').\n"
                    "7. **Visualizing Processes:** If a complex biological process is described (like a metabolic pathway), create a text-based flowchart using arrows ( -> ) to visualize the steps.\n\n"
                    f"Transcript:\n{transcript}"
                ]
            )
            msg = f"Markdown conversion successful for {file_path}"
            print(msg)
            logging.info(msg)
            # Increment and persist daily count
            daily_data["count"] += 1
            save_daily_count(daily_data["count"])
            return response.text.strip() + '\n'
        except Exception as e:
            err_msg = f"Error converting to markdown for {file_path} on attempt {attempt}: {e}"
            print(err_msg)
            logging.error(err_msg)
            
            # Check for rate limit errors
            error_str = str(e).lower()
            if 'rate limit' in error_str or 'quota' in error_str or '429' in error_str:
                wait_time = 60  # Wait 60 seconds for rate limit errors
                print(f"Rate limit detected. Waiting {wait_time} seconds...")
                logging.warning(f"Rate limit hit. Waiting {wait_time} seconds.")
                time.sleep(wait_time)
            elif attempt < max_retries:
                time.sleep(20)
            
            if attempt >= max_retries:
                logging.error(f"Max retries reached for {file_path}. Skipping.")
                print(f"Max retries reached for {file_path}. Skipping.")
    return None

def main():
    audio_extensions = {'.mp3', '.wav', '.m4a', '.aac', '.flac', '.ogg'}
    msg = f"Scanning '{BASE_AUDIO_DIR}' for audio files..."
    print(msg)
    logging.info(msg)

    # ...existing code...
    # Gather all audio files by channel
    channels = []
    for entry in os.scandir(BASE_AUDIO_DIR):
        if entry.is_dir():
            channels.append(entry.name)

    print(f"Found {len(channels)} channels.")
    logging.info(f"Found {len(channels)} channels.")

    # Resume checkpoint file
    checkpoint_file = Path(".transcribe_checkpoint")
    completed = set()
    if checkpoint_file.exists():
        with open(checkpoint_file, "r", encoding="utf-8") as f:
            completed = set(line.strip() for line in f if line.strip())
        print(f"Resuming from checkpoint. {len(completed)} files already completed.")
        logging.info(f"Resuming from checkpoint. {len(completed)} files already completed.")

    processed = 0
    for channel in channels:
        channel_dir = Path(BASE_AUDIO_DIR) / channel
        audio_files = []
        for file in channel_dir.glob("*.*"):
            if file.suffix.lower() in audio_extensions:
                audio_files.append(file)

        # Sort files newest first by date in filename (yyyy-mm-dd)
        def extract_date_from_filename(f):
            try:
                date_str = f.name.split('_')[0]
                return datetime.strptime(date_str, "%Y-%m-%d")
            except Exception:
                return datetime.min
        audio_files.sort(key=extract_date_from_filename, reverse=True)

        total_files = len(audio_files)
        print(f"Channel '{channel}': {total_files} audio files.")
        logging.info(f"Channel '{channel}': {total_files} audio files.")

        for idx, file_path in enumerate(audio_files, 1):
            rel_path_str = str(file_path.relative_to(BASE_AUDIO_DIR))
            if rel_path_str in completed:
                print(f"[{idx}/{total_files}] Skipping (already completed): {rel_path_str}")
                continue

            print(f"[{idx}/{total_files}] Processing: {rel_path_str}")
            logging.info(f"Processing {rel_path_str} ({idx}/{total_files})")

            relative_path = file_path.relative_to(BASE_AUDIO_DIR)

            # First, create simple transcription in subs/ folder
            subs_output_path = Path(BASE_SUBS_DIR) / relative_path.with_suffix('.txt')
            subs_output_path.parent.mkdir(parents=True, exist_ok=True)

            transcript = None
            if not subs_output_path.exists():
                transcript = transcribe_file_simple(file_path)
                if transcript:
                    with open(subs_output_path, "w", encoding="utf-8") as f:
                        f.write(transcript)
                    msg = f"Saved simple transcription: {subs_output_path}"
                    print(msg)
                    logging.info(msg)
                else:
                    err_msg = f"Failed to transcribe {file_path}"
                    print(err_msg)
                    logging.error(err_msg)
                    continue
            else:
                msg = f"Simple transcription already exists for {file_path.name}"
                print(msg)
                logging.info(msg)
                # Read transcript for markdown conversion
                with open(subs_output_path, "r", encoding="utf-8") as f:
                    transcript = f.read()

            # Second, create markdown version in md/ folder
            md_output_path = Path(BASE_MD_DIR) / relative_path.with_suffix('.md')
            md_output_path.parent.mkdir(parents=True, exist_ok=True)

            if not md_output_path.exists():
                if transcript:
                    markdown = convert_to_markdown(transcript, file_path)
                    if markdown:
                        with open(md_output_path, "w", encoding="utf-8") as f:
                            f.write(markdown)
                        msg = f"Saved markdown: {md_output_path}"
                        print(msg)
                        logging.info(msg)
                    else:
                        err_msg = f"Failed to create markdown for {file_path}"
                        print(err_msg)
                        logging.error(err_msg)
                else:
                    err_msg = f"No transcript available for markdown conversion: {file_path}"
                    print(err_msg)
                    logging.error(err_msg)
            else:
                msg = f"Markdown already exists for {file_path.name}"
                print(msg)
                logging.info(msg)

            # Mark as completed in checkpoint
            with open(checkpoint_file, "a", encoding="utf-8") as f:
                f.write(rel_path_str + "\n")
            processed += 1
            print(f"Progress: {processed + len(completed)}/{total_files} completed.")
    print(f"All done! {processed} new files processed, {len(completed)} previously completed.")
    logging.info(f"All done! {processed} new files processed, {len(completed)} previously completed.")

if __name__ == "__main__":
    main()
